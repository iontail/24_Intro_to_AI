{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ITnmEfj35zZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e00620dc-cf70-4f82-ea72-0c27e450478d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification, get_scheduler, BertForSequenceClassification\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import gdown\n",
        "import time"
      ],
      "metadata": {
        "id": "gqe7ERB66HN2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=12MOGiCveDE8CTvtHKqmEhyJIXc3gEscd'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = 'TwoStageDistilBERT_LoRA.pt'\n",
        "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "bert_checkpoint = 'skt/kobert-base-v1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "#bert_tokenizer = AutoTmokenizer.from_pretrained(bert_checkpoint)\n",
        "\n",
        "gdown.download(url, model_name, quiet = False)\n",
        "\n",
        "model_checkpoint = torch.load(model_name, map_location = device)"
      ],
      "metadata": {
        "id": "NHxSQiNz6HSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c89257c-bf1e-401b-b324-5b6e14e177ca"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=12MOGiCveDE8CTvtHKqmEhyJIXc3gEscd\n",
            "From (redirected): https://drive.google.com/uc?id=12MOGiCveDE8CTvtHKqmEhyJIXc3gEscd&confirm=t&uuid=48c5587b-505d-4f22-b4ba-3b2c2ee0c7dc\n",
            "To: /content/TwoStageDistilBERT_LoRA.pt\n",
            "100%|██████████| 893M/893M [00:10<00:00, 85.0MB/s]\n",
            "<ipython-input-27-fdf51cd4e783>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_checkpoint = torch.load(model_name, map_location = device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "bert_checkpoint = 'skt/kobert-base-v1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "yXNwj5l_A16i"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageDistilBERT_LoRA(nn.Module):\n",
        "  def __init__(self, distilbert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageDistilBERT_LoRA, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    lora_config1 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2"
      ],
      "metadata": {
        "id": "7H2iYRQN6HVs"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, model_checkpoint):\n",
        "  model.load_state_dict(model_checkpoint['model_state_dict'])\n",
        "\n",
        "  print(f\"Checkpoint loaded!\")\n",
        "  return model\n",
        "\n",
        "\n",
        "model = TwoStageDistilBERT_LoRA(distilbert_checkpoint = checkpoint)\n",
        "\n",
        "model = load_checkpoint(model, model_checkpoint)"
      ],
      "metadata": {
        "id": "Ho6uyRn66HYk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be408d35-bce6-4774-fcdf-f6169923b642"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['서울대 캠퍼스 입학 교육 대학 교수 학생 공지 연구 대학원 서울대학교 지원 도서관 서비스 미디어 월 행정 캘린더 센터 학사 뉴스 프로그램 학습 수 제 인스타그램 성과 구지원 학술 사항 안내 생활 관악 소식 소개 기념 역사 맵 가을 일 년 단 부문 영상 모습 회 중앙 예술 메뉴 일반']"
      ],
      "metadata": {
        "id": "D2OW414p6Hem"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://seungseop.tistory.com/41\n",
        "\n",
        "def model_inference(model, tokenizer, text):\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "\n",
        "  start_event = torch.cuda.Event(enable_timing = True)\n",
        "  end_event = torch.cuda.Event(enable_timing = True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "\n",
        "    # 텍스트를 토큰화\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    start_event.record()\n",
        "\n",
        "    #입력에 대한 추론 (추론에서는 gradient 필요없음)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # 1단계 모델에 입력\n",
        "    output1 = model.distilbert1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    hidden1 = output1.hidden_states[-1]  # 마지막 레이어의 hidden state\n",
        "    output2 = model.distilbert2(inputs_embeds=hidden1, attention_mask=attention_mask)\n",
        "\n",
        "    end_event.record()\n",
        "\n",
        "  torch.cuda.synchronize()\n",
        "\n",
        "  time_taken = start_event.elapsed_time(end_event)\n",
        "  return time_taken"
      ],
      "metadata": {
        "id": "Aw9TTcTj7qLU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_inference = model_inference(model, tokenizer, text)\n",
        "print(f\"Elapsed time on GPU: {distilbert_inference} mile seconds\")"
      ],
      "metadata": {
        "id": "98SxXx-D7qOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09748bf-dcc4-4967-f1c7-8c78f0fb1c1e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time on GPU: 30.415199279785156 mile seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in BertForSequenceClassification.from_pretrained(bert_checkpoint,\n",
        "                                                                           num_labels = 2, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True).named_modules():\n",
        "  print(name)"
      ],
      "metadata": {
        "id": "9tRLkc33AIw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2168259-9e6a-4fd7-b044-4cf1e0117f03"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bert\n",
            "bert.embeddings\n",
            "bert.embeddings.word_embeddings\n",
            "bert.embeddings.position_embeddings\n",
            "bert.embeddings.token_type_embeddings\n",
            "bert.embeddings.LayerNorm\n",
            "bert.embeddings.dropout\n",
            "bert.encoder\n",
            "bert.encoder.layer\n",
            "bert.encoder.layer.0\n",
            "bert.encoder.layer.0.attention\n",
            "bert.encoder.layer.0.attention.self\n",
            "bert.encoder.layer.0.attention.self.query\n",
            "bert.encoder.layer.0.attention.self.key\n",
            "bert.encoder.layer.0.attention.self.value\n",
            "bert.encoder.layer.0.attention.self.dropout\n",
            "bert.encoder.layer.0.attention.output\n",
            "bert.encoder.layer.0.attention.output.dense\n",
            "bert.encoder.layer.0.attention.output.LayerNorm\n",
            "bert.encoder.layer.0.attention.output.dropout\n",
            "bert.encoder.layer.0.intermediate\n",
            "bert.encoder.layer.0.intermediate.dense\n",
            "bert.encoder.layer.0.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.0.output\n",
            "bert.encoder.layer.0.output.dense\n",
            "bert.encoder.layer.0.output.LayerNorm\n",
            "bert.encoder.layer.0.output.dropout\n",
            "bert.encoder.layer.1\n",
            "bert.encoder.layer.1.attention\n",
            "bert.encoder.layer.1.attention.self\n",
            "bert.encoder.layer.1.attention.self.query\n",
            "bert.encoder.layer.1.attention.self.key\n",
            "bert.encoder.layer.1.attention.self.value\n",
            "bert.encoder.layer.1.attention.self.dropout\n",
            "bert.encoder.layer.1.attention.output\n",
            "bert.encoder.layer.1.attention.output.dense\n",
            "bert.encoder.layer.1.attention.output.LayerNorm\n",
            "bert.encoder.layer.1.attention.output.dropout\n",
            "bert.encoder.layer.1.intermediate\n",
            "bert.encoder.layer.1.intermediate.dense\n",
            "bert.encoder.layer.1.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.1.output\n",
            "bert.encoder.layer.1.output.dense\n",
            "bert.encoder.layer.1.output.LayerNorm\n",
            "bert.encoder.layer.1.output.dropout\n",
            "bert.encoder.layer.2\n",
            "bert.encoder.layer.2.attention\n",
            "bert.encoder.layer.2.attention.self\n",
            "bert.encoder.layer.2.attention.self.query\n",
            "bert.encoder.layer.2.attention.self.key\n",
            "bert.encoder.layer.2.attention.self.value\n",
            "bert.encoder.layer.2.attention.self.dropout\n",
            "bert.encoder.layer.2.attention.output\n",
            "bert.encoder.layer.2.attention.output.dense\n",
            "bert.encoder.layer.2.attention.output.LayerNorm\n",
            "bert.encoder.layer.2.attention.output.dropout\n",
            "bert.encoder.layer.2.intermediate\n",
            "bert.encoder.layer.2.intermediate.dense\n",
            "bert.encoder.layer.2.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.2.output\n",
            "bert.encoder.layer.2.output.dense\n",
            "bert.encoder.layer.2.output.LayerNorm\n",
            "bert.encoder.layer.2.output.dropout\n",
            "bert.encoder.layer.3\n",
            "bert.encoder.layer.3.attention\n",
            "bert.encoder.layer.3.attention.self\n",
            "bert.encoder.layer.3.attention.self.query\n",
            "bert.encoder.layer.3.attention.self.key\n",
            "bert.encoder.layer.3.attention.self.value\n",
            "bert.encoder.layer.3.attention.self.dropout\n",
            "bert.encoder.layer.3.attention.output\n",
            "bert.encoder.layer.3.attention.output.dense\n",
            "bert.encoder.layer.3.attention.output.LayerNorm\n",
            "bert.encoder.layer.3.attention.output.dropout\n",
            "bert.encoder.layer.3.intermediate\n",
            "bert.encoder.layer.3.intermediate.dense\n",
            "bert.encoder.layer.3.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.3.output\n",
            "bert.encoder.layer.3.output.dense\n",
            "bert.encoder.layer.3.output.LayerNorm\n",
            "bert.encoder.layer.3.output.dropout\n",
            "bert.encoder.layer.4\n",
            "bert.encoder.layer.4.attention\n",
            "bert.encoder.layer.4.attention.self\n",
            "bert.encoder.layer.4.attention.self.query\n",
            "bert.encoder.layer.4.attention.self.key\n",
            "bert.encoder.layer.4.attention.self.value\n",
            "bert.encoder.layer.4.attention.self.dropout\n",
            "bert.encoder.layer.4.attention.output\n",
            "bert.encoder.layer.4.attention.output.dense\n",
            "bert.encoder.layer.4.attention.output.LayerNorm\n",
            "bert.encoder.layer.4.attention.output.dropout\n",
            "bert.encoder.layer.4.intermediate\n",
            "bert.encoder.layer.4.intermediate.dense\n",
            "bert.encoder.layer.4.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.4.output\n",
            "bert.encoder.layer.4.output.dense\n",
            "bert.encoder.layer.4.output.LayerNorm\n",
            "bert.encoder.layer.4.output.dropout\n",
            "bert.encoder.layer.5\n",
            "bert.encoder.layer.5.attention\n",
            "bert.encoder.layer.5.attention.self\n",
            "bert.encoder.layer.5.attention.self.query\n",
            "bert.encoder.layer.5.attention.self.key\n",
            "bert.encoder.layer.5.attention.self.value\n",
            "bert.encoder.layer.5.attention.self.dropout\n",
            "bert.encoder.layer.5.attention.output\n",
            "bert.encoder.layer.5.attention.output.dense\n",
            "bert.encoder.layer.5.attention.output.LayerNorm\n",
            "bert.encoder.layer.5.attention.output.dropout\n",
            "bert.encoder.layer.5.intermediate\n",
            "bert.encoder.layer.5.intermediate.dense\n",
            "bert.encoder.layer.5.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.5.output\n",
            "bert.encoder.layer.5.output.dense\n",
            "bert.encoder.layer.5.output.LayerNorm\n",
            "bert.encoder.layer.5.output.dropout\n",
            "bert.encoder.layer.6\n",
            "bert.encoder.layer.6.attention\n",
            "bert.encoder.layer.6.attention.self\n",
            "bert.encoder.layer.6.attention.self.query\n",
            "bert.encoder.layer.6.attention.self.key\n",
            "bert.encoder.layer.6.attention.self.value\n",
            "bert.encoder.layer.6.attention.self.dropout\n",
            "bert.encoder.layer.6.attention.output\n",
            "bert.encoder.layer.6.attention.output.dense\n",
            "bert.encoder.layer.6.attention.output.LayerNorm\n",
            "bert.encoder.layer.6.attention.output.dropout\n",
            "bert.encoder.layer.6.intermediate\n",
            "bert.encoder.layer.6.intermediate.dense\n",
            "bert.encoder.layer.6.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.6.output\n",
            "bert.encoder.layer.6.output.dense\n",
            "bert.encoder.layer.6.output.LayerNorm\n",
            "bert.encoder.layer.6.output.dropout\n",
            "bert.encoder.layer.7\n",
            "bert.encoder.layer.7.attention\n",
            "bert.encoder.layer.7.attention.self\n",
            "bert.encoder.layer.7.attention.self.query\n",
            "bert.encoder.layer.7.attention.self.key\n",
            "bert.encoder.layer.7.attention.self.value\n",
            "bert.encoder.layer.7.attention.self.dropout\n",
            "bert.encoder.layer.7.attention.output\n",
            "bert.encoder.layer.7.attention.output.dense\n",
            "bert.encoder.layer.7.attention.output.LayerNorm\n",
            "bert.encoder.layer.7.attention.output.dropout\n",
            "bert.encoder.layer.7.intermediate\n",
            "bert.encoder.layer.7.intermediate.dense\n",
            "bert.encoder.layer.7.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.7.output\n",
            "bert.encoder.layer.7.output.dense\n",
            "bert.encoder.layer.7.output.LayerNorm\n",
            "bert.encoder.layer.7.output.dropout\n",
            "bert.encoder.layer.8\n",
            "bert.encoder.layer.8.attention\n",
            "bert.encoder.layer.8.attention.self\n",
            "bert.encoder.layer.8.attention.self.query\n",
            "bert.encoder.layer.8.attention.self.key\n",
            "bert.encoder.layer.8.attention.self.value\n",
            "bert.encoder.layer.8.attention.self.dropout\n",
            "bert.encoder.layer.8.attention.output\n",
            "bert.encoder.layer.8.attention.output.dense\n",
            "bert.encoder.layer.8.attention.output.LayerNorm\n",
            "bert.encoder.layer.8.attention.output.dropout\n",
            "bert.encoder.layer.8.intermediate\n",
            "bert.encoder.layer.8.intermediate.dense\n",
            "bert.encoder.layer.8.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.8.output\n",
            "bert.encoder.layer.8.output.dense\n",
            "bert.encoder.layer.8.output.LayerNorm\n",
            "bert.encoder.layer.8.output.dropout\n",
            "bert.encoder.layer.9\n",
            "bert.encoder.layer.9.attention\n",
            "bert.encoder.layer.9.attention.self\n",
            "bert.encoder.layer.9.attention.self.query\n",
            "bert.encoder.layer.9.attention.self.key\n",
            "bert.encoder.layer.9.attention.self.value\n",
            "bert.encoder.layer.9.attention.self.dropout\n",
            "bert.encoder.layer.9.attention.output\n",
            "bert.encoder.layer.9.attention.output.dense\n",
            "bert.encoder.layer.9.attention.output.LayerNorm\n",
            "bert.encoder.layer.9.attention.output.dropout\n",
            "bert.encoder.layer.9.intermediate\n",
            "bert.encoder.layer.9.intermediate.dense\n",
            "bert.encoder.layer.9.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.9.output\n",
            "bert.encoder.layer.9.output.dense\n",
            "bert.encoder.layer.9.output.LayerNorm\n",
            "bert.encoder.layer.9.output.dropout\n",
            "bert.encoder.layer.10\n",
            "bert.encoder.layer.10.attention\n",
            "bert.encoder.layer.10.attention.self\n",
            "bert.encoder.layer.10.attention.self.query\n",
            "bert.encoder.layer.10.attention.self.key\n",
            "bert.encoder.layer.10.attention.self.value\n",
            "bert.encoder.layer.10.attention.self.dropout\n",
            "bert.encoder.layer.10.attention.output\n",
            "bert.encoder.layer.10.attention.output.dense\n",
            "bert.encoder.layer.10.attention.output.LayerNorm\n",
            "bert.encoder.layer.10.attention.output.dropout\n",
            "bert.encoder.layer.10.intermediate\n",
            "bert.encoder.layer.10.intermediate.dense\n",
            "bert.encoder.layer.10.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.10.output\n",
            "bert.encoder.layer.10.output.dense\n",
            "bert.encoder.layer.10.output.LayerNorm\n",
            "bert.encoder.layer.10.output.dropout\n",
            "bert.encoder.layer.11\n",
            "bert.encoder.layer.11.attention\n",
            "bert.encoder.layer.11.attention.self\n",
            "bert.encoder.layer.11.attention.self.query\n",
            "bert.encoder.layer.11.attention.self.key\n",
            "bert.encoder.layer.11.attention.self.value\n",
            "bert.encoder.layer.11.attention.self.dropout\n",
            "bert.encoder.layer.11.attention.output\n",
            "bert.encoder.layer.11.attention.output.dense\n",
            "bert.encoder.layer.11.attention.output.LayerNorm\n",
            "bert.encoder.layer.11.attention.output.dropout\n",
            "bert.encoder.layer.11.intermediate\n",
            "bert.encoder.layer.11.intermediate.dense\n",
            "bert.encoder.layer.11.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.11.output\n",
            "bert.encoder.layer.11.output.dense\n",
            "bert.encoder.layer.11.output.LayerNorm\n",
            "bert.encoder.layer.11.output.dropout\n",
            "bert.pooler\n",
            "bert.pooler.dense\n",
            "bert.pooler.activation\n",
            "dropout\n",
            "classifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageBERT_LoRA(nn.Module):\n",
        "  def __init__(self, bert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageBERT_LoRA, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = BertForSequenceClassification.from_pretrained(bert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    lora_config1 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['query', 'value'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = BertForSequenceClassification.from_pretrained(bert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2"
      ],
      "metadata": {
        "id": "OD8HWIGI7qY7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = TwoStageBERT_LoRA(bert_checkpoint)"
      ],
      "metadata": {
        "id": "t8tD_y2u6Hhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f999ad56-fcbd-486d-b48e-86b7a64d77a5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at skt/kobert-base-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_checkpoint = 'skt/kobert-base-v1'\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
        "\n",
        "\n",
        "bert_inference = model_inference(bert_model, bert_tokenizer, text)\n",
        "print(f\"Elapsed time on GPU: {bert_inference } mile seconds\")"
      ],
      "metadata": {
        "id": "Q7F_dnzrAHbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e060fe8-ef6b-4c9f-d268-297b697e2af5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time on GPU: 41.01478576660156 mile seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageDistilBERT_LoRA1(nn.Module):\n",
        "  def __init__(self, distilbert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageDistilBERT_LoRA1, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    lora_config1 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2"
      ],
      "metadata": {
        "id": "Kohyt8i6ifzP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_lora1_model = TwoStageDistilBERT_LoRA1(distilbert_checkpoint = checkpoint)\n",
        "\n",
        "distilbert_lora1_inference = model_inference(distilbert_lora1_model, tokenizer, text)\n",
        "print(f\"Elapsed time on GPU: {distilbert_lora1_inference} mile seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fDC12sJiw0e",
        "outputId": "3eb7c448-7270-4930-f845-1d34510ac1ec"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time on GPU: 44.09775924682617 mile seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageDistilBERT(nn.Module):\n",
        "  def __init__(self, distilbert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageDistilBERT, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2"
      ],
      "metadata": {
        "id": "H9JC4Lr3tGz8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_model = TwoStageDistilBERT(distilbert_checkpoint = checkpoint)\n",
        "\n",
        "distilbert_full = model_inference(distilbert_model, tokenizer, text)\n",
        "print(f\"Elapsed time on GPU: {distilbert_full} mile seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoiKU9NrtJE2",
        "outputId": "905ea27a-dd21-489f-be57-711bab4c3065"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elapsed time on GPU: 29.535999298095703 mile seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'DistilBERT: {distilbert_inference:.3f}ms, BERT: {bert_inference:.3f}ms')\n",
        "print(f'DistilBERT model is {bert_inference /distilbert_inference:.1f} times faster')"
      ],
      "metadata": {
        "id": "EvLN96n2_UK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce35e08-affb-4739-916f-14008e6b5b5e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT: 30.415ms, BERT: 41.015ms\n",
            "DistilBERT model is 1.3 times faster\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "b = []\n",
        "c = []\n",
        "d = []\n",
        "for i in range(100):\n",
        "  bert_inference = model_inference(bert_model, bert_tokenizer, text)\n",
        "  distilbert_lora1_inference = model_inference(distilbert_lora1_model, tokenizer, text)\n",
        "  distilbert_inference = model_inference(model, tokenizer, text)\n",
        "  distilbert_full_inference = model_inference(distilbert_model, tokenizer, text)\n",
        "  c.append(bert_inference)\n",
        "  b.append(distilbert_lora1_inference)\n",
        "  a.append(distilbert_inference)\n",
        "  d.append(distilbert_full_inference)\n",
        "  print(f'DistilBERT: {distilbert_inference:.3f}ms, LoRA_1: {distilbert_lora1_inference:.3f}ms, BERT: {bert_inference:.3f}ms')\n",
        "  print(f'DistilBERT model is {bert_inference /distilbert_inference:.1f} times faster')"
      ],
      "metadata": {
        "id": "ldOaGZDXDQR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "193a87b0-2696-4c45-c201-d255f8fa6d5e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBERT: 30.112ms, LoRA_1: 29.790ms, BERT: 72.833ms\n",
            "DistilBERT model is 2.4 times faster\n",
            "DistilBERT: 50.321ms, LoRA_1: 29.688ms, BERT: 42.810ms\n",
            "DistilBERT model is 0.9 times faster\n",
            "DistilBERT: 19.339ms, LoRA_1: 46.926ms, BERT: 66.823ms\n",
            "DistilBERT model is 3.5 times faster\n",
            "DistilBERT: 78.305ms, LoRA_1: 19.145ms, BERT: 67.505ms\n",
            "DistilBERT model is 0.9 times faster\n",
            "DistilBERT: 18.748ms, LoRA_1: 18.339ms, BERT: 28.804ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 18.252ms, LoRA_1: 18.264ms, BERT: 25.021ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 19.244ms, LoRA_1: 17.480ms, BERT: 33.089ms\n",
            "DistilBERT model is 1.7 times faster\n",
            "DistilBERT: 19.331ms, LoRA_1: 19.230ms, BERT: 24.984ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.104ms, LoRA_1: 18.336ms, BERT: 23.650ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 18.989ms, LoRA_1: 19.139ms, BERT: 23.387ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 18.078ms, LoRA_1: 17.194ms, BERT: 26.301ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 19.091ms, LoRA_1: 18.705ms, BERT: 23.605ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.145ms, LoRA_1: 18.342ms, BERT: 23.375ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 20.346ms, LoRA_1: 19.763ms, BERT: 23.917ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.938ms, LoRA_1: 18.722ms, BERT: 23.851ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.120ms, LoRA_1: 19.200ms, BERT: 24.598ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 16.265ms, LoRA_1: 16.146ms, BERT: 32.984ms\n",
            "DistilBERT model is 2.0 times faster\n",
            "DistilBERT: 18.180ms, LoRA_1: 17.604ms, BERT: 24.500ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.237ms, LoRA_1: 18.378ms, BERT: 23.360ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.634ms, LoRA_1: 19.207ms, BERT: 24.259ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.147ms, LoRA_1: 18.242ms, BERT: 23.019ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.082ms, LoRA_1: 19.241ms, BERT: 23.673ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 18.244ms, LoRA_1: 18.836ms, BERT: 23.218ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.649ms, LoRA_1: 18.061ms, BERT: 25.406ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 19.872ms, LoRA_1: 19.827ms, BERT: 23.947ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 17.826ms, LoRA_1: 18.408ms, BERT: 25.151ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 16.927ms, LoRA_1: 18.809ms, BERT: 24.324ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 18.448ms, LoRA_1: 17.884ms, BERT: 23.710ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 20.972ms, LoRA_1: 19.368ms, BERT: 23.604ms\n",
            "DistilBERT model is 1.1 times faster\n",
            "DistilBERT: 17.539ms, LoRA_1: 17.501ms, BERT: 26.970ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 18.974ms, LoRA_1: 18.735ms, BERT: 23.474ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.769ms, LoRA_1: 18.827ms, BERT: 23.939ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 18.267ms, LoRA_1: 18.205ms, BERT: 24.371ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.814ms, LoRA_1: 19.774ms, BERT: 23.508ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 20.816ms, LoRA_1: 20.376ms, BERT: 24.127ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 17.504ms, LoRA_1: 17.591ms, BERT: 26.127ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 18.250ms, LoRA_1: 19.153ms, BERT: 23.962ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.381ms, LoRA_1: 17.898ms, BERT: 23.955ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.441ms, LoRA_1: 19.200ms, BERT: 23.721ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.552ms, LoRA_1: 19.747ms, BERT: 23.931ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 21.543ms, LoRA_1: 18.274ms, BERT: 24.193ms\n",
            "DistilBERT model is 1.1 times faster\n",
            "DistilBERT: 18.425ms, LoRA_1: 17.967ms, BERT: 24.446ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.479ms, LoRA_1: 19.243ms, BERT: 23.967ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 18.486ms, LoRA_1: 18.465ms, BERT: 24.493ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 17.263ms, LoRA_1: 18.186ms, BERT: 25.309ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 18.164ms, LoRA_1: 17.752ms, BERT: 24.838ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 18.260ms, LoRA_1: 17.505ms, BERT: 23.964ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.907ms, LoRA_1: 19.081ms, BERT: 27.071ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 20.445ms, LoRA_1: 18.173ms, BERT: 26.443ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.589ms, LoRA_1: 19.709ms, BERT: 24.330ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.593ms, LoRA_1: 19.174ms, BERT: 23.525ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.247ms, LoRA_1: 18.340ms, BERT: 23.918ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 19.055ms, LoRA_1: 20.248ms, BERT: 24.205ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.511ms, LoRA_1: 18.614ms, BERT: 24.028ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.411ms, LoRA_1: 19.368ms, BERT: 24.179ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 18.047ms, LoRA_1: 18.774ms, BERT: 24.557ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 18.562ms, LoRA_1: 18.045ms, BERT: 23.847ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 17.149ms, LoRA_1: 17.271ms, BERT: 26.870ms\n",
            "DistilBERT model is 1.6 times faster\n",
            "DistilBERT: 18.400ms, LoRA_1: 17.948ms, BERT: 24.230ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.253ms, LoRA_1: 18.235ms, BERT: 25.063ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 18.756ms, LoRA_1: 19.290ms, BERT: 24.783ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 17.062ms, LoRA_1: 17.332ms, BERT: 29.008ms\n",
            "DistilBERT model is 1.7 times faster\n",
            "DistilBERT: 18.059ms, LoRA_1: 17.717ms, BERT: 25.987ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 20.427ms, LoRA_1: 19.114ms, BERT: 26.626ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.638ms, LoRA_1: 18.676ms, BERT: 25.666ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 19.519ms, LoRA_1: 19.720ms, BERT: 24.107ms\n",
            "DistilBERT model is 1.2 times faster\n",
            "DistilBERT: 17.319ms, LoRA_1: 18.973ms, BERT: 24.844ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 17.711ms, LoRA_1: 18.077ms, BERT: 24.323ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 17.860ms, LoRA_1: 17.553ms, BERT: 24.019ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 19.062ms, LoRA_1: 19.206ms, BERT: 24.003ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 22.307ms, LoRA_1: 17.538ms, BERT: 40.801ms\n",
            "DistilBERT model is 1.8 times faster\n",
            "DistilBERT: 19.941ms, LoRA_1: 16.294ms, BERT: 30.290ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 18.253ms, LoRA_1: 19.226ms, BERT: 26.141ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 17.228ms, LoRA_1: 18.209ms, BERT: 30.822ms\n",
            "DistilBERT model is 1.8 times faster\n",
            "DistilBERT: 20.544ms, LoRA_1: 17.370ms, BERT: 26.430ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 22.300ms, LoRA_1: 20.091ms, BERT: 30.098ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.952ms, LoRA_1: 20.005ms, BERT: 26.017ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 17.509ms, LoRA_1: 15.894ms, BERT: 26.091ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 18.478ms, LoRA_1: 18.972ms, BERT: 27.107ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 17.696ms, LoRA_1: 16.335ms, BERT: 25.312ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 20.169ms, LoRA_1: 16.882ms, BERT: 31.783ms\n",
            "DistilBERT model is 1.6 times faster\n",
            "DistilBERT: 17.476ms, LoRA_1: 19.661ms, BERT: 27.631ms\n",
            "DistilBERT model is 1.6 times faster\n",
            "DistilBERT: 19.974ms, LoRA_1: 18.052ms, BERT: 29.757ms\n",
            "DistilBERT model is 1.5 times faster\n",
            "DistilBERT: 18.746ms, LoRA_1: 19.571ms, BERT: 32.906ms\n",
            "DistilBERT model is 1.8 times faster\n",
            "DistilBERT: 21.809ms, LoRA_1: 21.365ms, BERT: 29.664ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 18.227ms, LoRA_1: 17.958ms, BERT: 34.498ms\n",
            "DistilBERT model is 1.9 times faster\n",
            "DistilBERT: 20.826ms, LoRA_1: 16.647ms, BERT: 28.606ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 21.717ms, LoRA_1: 16.808ms, BERT: 28.936ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 20.326ms, LoRA_1: 19.018ms, BERT: 29.270ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 17.601ms, LoRA_1: 18.017ms, BERT: 31.035ms\n",
            "DistilBERT model is 1.8 times faster\n",
            "DistilBERT: 17.605ms, LoRA_1: 33.931ms, BERT: 39.510ms\n",
            "DistilBERT model is 2.2 times faster\n",
            "DistilBERT: 19.463ms, LoRA_1: 18.823ms, BERT: 34.461ms\n",
            "DistilBERT model is 1.8 times faster\n",
            "DistilBERT: 20.018ms, LoRA_1: 17.738ms, BERT: 31.648ms\n",
            "DistilBERT model is 1.6 times faster\n",
            "DistilBERT: 16.188ms, LoRA_1: 15.917ms, BERT: 29.991ms\n",
            "DistilBERT model is 1.9 times faster\n",
            "DistilBERT: 16.384ms, LoRA_1: 16.803ms, BERT: 30.175ms\n",
            "DistilBERT model is 1.8 times faster\n",
            "DistilBERT: 17.504ms, LoRA_1: 17.824ms, BERT: 24.455ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 18.921ms, LoRA_1: 19.157ms, BERT: 24.637ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.969ms, LoRA_1: 18.899ms, BERT: 24.213ms\n",
            "DistilBERT model is 1.3 times faster\n",
            "DistilBERT: 18.573ms, LoRA_1: 18.961ms, BERT: 25.626ms\n",
            "DistilBERT model is 1.4 times faster\n",
            "DistilBERT: 18.192ms, LoRA_1: 18.465ms, BERT: 26.874ms\n",
            "DistilBERT model is 1.5 times faster\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(a) / 10)\n",
        "print(sum(b) / 10)\n",
        "print(sum(c) / 10)\n",
        "print(sum(d) / 10)\n",
        "\n",
        "print(f'{sum(c) / sum(a):.3f}')\n",
        "print(f'{(sum(c) / sum(a) - 1.0) * 100 :.1f}')\n",
        "print(f'{(sum(a) / sum(c)) * 100 :.1f}')\n"
      ],
      "metadata": {
        "id": "PpMN6chDDQRD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ffbe359-7687-482b-eae1-eb0fe0a1b383"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "199.13504009246827\n",
            "191.17423677444458\n",
            "277.5424381256104\n",
            "183.76384687423706\n",
            "1.394\n",
            "39.4\n",
            "71.7\n"
          ]
        }
      ]
    }
  ]
}