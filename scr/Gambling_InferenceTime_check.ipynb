{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITnmEfj35zZF"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification, get_scheduler, BertForSequenceClassification\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import gdown\n",
        "import time"
      ],
      "metadata": {
        "id": "gqe7ERB66HN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=12MOGiCveDE8CTvtHKqmEhyJIXc3gEscd'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_name = 'TwoStageDistilBERT_LoRA.pt'\n",
        "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "bert_checkpoint = 'skt/kobert-base-v1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "#bert_tokenizer = AutoTmokenizer.from_pretrained(bert_checkpoint)\n",
        "\n",
        "gdown.download(url, model_name, quiet = False)\n",
        "\n",
        "model_checkpoint = torch.load(model_name, map_location = device)"
      ],
      "metadata": {
        "id": "NHxSQiNz6HSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "bert_checkpoint = 'skt/kobert-base-v1'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "yXNwj5l_A16i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageDistilBERT_LoRA(nn.Module):\n",
        "  def __init__(self, distilbert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageDistilBERT_LoRA, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    lora_config1 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "    lora_config2 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config2)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2"
      ],
      "metadata": {
        "id": "7H2iYRQN6HVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, model_checkpoint):\n",
        "  model.load_state_dict(model_checkpoint['model_state_dict'])\n",
        "\n",
        "  print(f\"Checkpoint loaded!\")\n",
        "  return model\n",
        "\n",
        "\n",
        "model = TwoStageDistilBERT_LoRA(distilbert_checkpoint = checkpoint)\n",
        "\n",
        "model = load_checkpoint(model, model_checkpoint)"
      ],
      "metadata": {
        "id": "Ho6uyRn66HYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['서울대 캠퍼스 입학 교육 대학 교수 학생 공지 연구 대학원 서울대학교 지원 도서관 서비스 미디어 월 행정 캘린더 센터 학사 뉴스 프로그램 학습 수 제 인스타그램 성과 구지원 학술 사항 안내 생활 관악 소식 소개 기념 역사 맵 가을 일 년 단 부문 영상 모습 회 중앙 예술 메뉴 일반']"
      ],
      "metadata": {
        "id": "D2OW414p6Hem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://seungseop.tistory.com/41\n",
        "\n",
        "def model_inference(model, tokenizer, text):\n",
        "  model.eval()\n",
        "  model = model.to(device)\n",
        "\n",
        "  start_event = torch.cuda.Event(enable_timing = True)\n",
        "  end_event = torch.cuda.Event(enable_timing = True)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "\n",
        "    # 텍스트를 토큰화\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    start_event.record()\n",
        "\n",
        "    #입력에 대한 추론 (추론에서는 gradient 필요없음)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # 1단계 모델에 입력\n",
        "    output1 = model.distilbert1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    hidden1 = output1.hidden_states[-1]  # 마지막 레이어의 hidden state\n",
        "    output2 = model.distilbert2(inputs_embeds=hidden1, attention_mask=attention_mask)\n",
        "\n",
        "    end_event.record()\n",
        "\n",
        "  torch.cuda.synchronize()\n",
        "\n",
        "  time_taken = start_event.elapsed_time(end_event)\n",
        "  return time_taken"
      ],
      "metadata": {
        "id": "Aw9TTcTj7qLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distilbert_inference = model_inference(model, tokenizer, text)\n",
        "print(f\"Elapsed time on GPU: {distilbert_inference} mile seconds\")"
      ],
      "metadata": {
        "id": "98SxXx-D7qOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in BertForSequenceClassification.from_pretrained(bert_checkpoint,\n",
        "                                                                           num_labels = 2, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True).named_modules():\n",
        "  print(name)"
      ],
      "metadata": {
        "id": "9tRLkc33AIw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageBERT_LoRA(nn.Module):\n",
        "  def __init__(self, bert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageBERT_LoRA, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = BertForSequenceClassification.from_pretrained(bert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    lora_config1 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['query', 'value'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = BertForSequenceClassification.from_pretrained(bert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "    lora_config2 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['query', 'value'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config2)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2"
      ],
      "metadata": {
        "id": "OD8HWIGI7qY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = TwoStageBERT_LoRA(bert_checkpoint)"
      ],
      "metadata": {
        "id": "t8tD_y2u6Hhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_checkpoint = 'skt/kobert-base-v1'\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
        "\n",
        "\n",
        "bert_inference = model_inference(bert_model, bert_tokenizer, text)\n",
        "print(f\"Elapsed time on GPU: {bert_inference } mile seconds\")"
      ],
      "metadata": {
        "id": "Q7F_dnzrAHbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'DistilBERT: {distilbert_inference:.3f}ms, BERT: {bert_inference:.3f}ms')\n",
        "print(f'DistilBERT model is {bert_inference /distilbert_inference:.1f} times faster')"
      ],
      "metadata": {
        "id": "EvLN96n2_UK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = []\n",
        "b = []\n",
        "for i in range(100):\n",
        "  bert_inference = model_inference(bert_model, bert_tokenizer, text)\n",
        "  distilbert_inference = model_inference(model, tokenizer, text)\n",
        "  a.append(bert_inference)\n",
        "  b.append(distilbert_inference)\n",
        "  print(f'DistilBERT: {distilbert_inference:.3f}ms, BERT: {bert_inference:.3f}ms')\n",
        "  print(f'DistilBERT model is {bert_inference /distilbert_inference:.1f} times faster')"
      ],
      "metadata": {
        "id": "ldOaGZDXDQR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(b) / 10)\n",
        "print(sum(a) / 10)\n",
        "\n",
        "print(f'{sum(a) / sum(b):.3f}')\n",
        "print(f'{(sum(a) / sum(b) - 1.0) * 100 :.1f}')\n",
        "print(f'{(sum(b) / sum(a)) * 100 :.1f}')\n"
      ],
      "metadata": {
        "id": "PpMN6chDDQRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3518v-s4_UQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7iX_D-RZ_UVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ig-5_qJC_UZg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}