{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1k0tm5Lktmo"
      },
      "outputs": [],
      "source": [
        "#코랩 셀레니움 설치\n",
        "!apt-get update\n",
        "!apt-get install -y chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "#konlpy 설치. 3분정도 걸려요\n",
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install peft\n",
        "!pip install gdown"
      ],
      "metadata": {
        "id": "vSekDDP-aSAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification, get_scheduler\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import gdown\n",
        "\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from konlpy.tag import Okt\n",
        "from selenium import webdriver"
      ],
      "metadata": {
        "id": "AQWr71xzaR9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=12MOGiCveDE8CTvtHKqmEhyJIXc3gEscd'\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "model_name = 'TwoStageDistilBERT_LoRA.pt'\n",
        "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "gdown.download(url, model_name, quiet = False)\n",
        "\n",
        "model_checkpoint = torch.load(model_name, map_location = device)"
      ],
      "metadata": {
        "id": "GeIrrgY7aR1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoStageDistilBERT_LoRA(nn.Module):\n",
        "  def __init__(self, distilbert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageDistilBERT_LoRA, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    lora_config1 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "    lora_config2 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config2)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2"
      ],
      "metadata": {
        "id": "eAYeZ1nvaRxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, model_checkpoint):\n",
        "  model.load_state_dict(model_checkpoint['model_state_dict'])\n",
        "\n",
        "  print(f\"Checkpoint loaded!\")\n",
        "  return model\n",
        "\n",
        "\n",
        "model = TwoStageDistilBERT_LoRA(distilbert_checkpoint = checkpoint)\n",
        "\n",
        "model = load_checkpoint(model, model_checkpoint)"
      ],
      "metadata": {
        "id": "EmEO-4_oeiis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output2label(logits1, logits2, label_to_site_dict):\n",
        "  N = logits1.shape[0]\n",
        "  logits1 = np.array(logits1)\n",
        "  logits2 = np.array(logits2)\n",
        "\n",
        "\n",
        "\n",
        "  output1 = np.argmax(logits1, axis = 1)\n",
        "  #print(output1, \"\\n\",[label_to_site_dict[out] for out in output1])\n",
        "\n",
        "  mask = (output1 != 0)\n",
        "  output2 = np.argmax(logits2[mask], axis = 1)\n",
        "  #print(output2, \"\\n\", [label_to_site_dict[out] for out in output2])\n",
        "\n",
        "  result = []\n",
        "  idx = 0\n",
        "  for out in output1:\n",
        "    if out == 0:\n",
        "      result.append(label_to_site_dict[out])\n",
        "    else:\n",
        "      result.append(label_to_site_dict[output2[idx]])\n",
        "      idx += 1\n",
        "\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "FVEZ1VdveLsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#셀레니움 설정\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--disable-blink-features=AutomationControlled')"
      ],
      "metadata": {
        "id": "XQUsPsS-aRsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraction(url):\n",
        "  driver = webdriver.Chrome(options=options)\n",
        "  driver.get(url)\n",
        "  time.sleep(5)#셀레니움이 해당 웹사이트까지 이동하는데 기다려줘야함\n",
        "\n",
        "  html = driver.page_source #html긁어오기\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  texts= soup.get_text()#텍스트만 추출\n",
        "  driver.quit()#셀레니움 종료\n",
        "\n",
        "  okt=Okt()\n",
        "  pattern = re.compile('[ㄱ-ㅎ가-힣]+')#한글만\n",
        "  korean=pattern.findall(texts)\n",
        "  nouns_list=[] #명사만 추출해서 리스트에 넣을예정\n",
        "  for text in korean:\n",
        "    noun = okt.nouns(text) #ex) '페이지를'->'페이지'\n",
        "    for i in noun:\n",
        "      nouns_list.append(i)# 명사 append\n",
        "\n",
        "  bow={} #빈도수 체크 딕셔너리\n",
        "  for i in nouns_list:\n",
        "    if i not in bow.keys():\n",
        "      bow[i]=1\n",
        "    else:\n",
        "      bow[i]+=1\n",
        "  bow_list=list(zip(bow.keys(),bow.values()))\n",
        "  bow_list.sort(key=lambda x:x[1],reverse=True)#내림차순 정렬\n",
        "\n",
        "  keyword=[]\n",
        "  for i in range(len(bow_list)):\n",
        "    if i==50:#키워드가 50개보다 많을경우 50개까지만\n",
        "      break\n",
        "    else:\n",
        "      keyword.append(bow_list[i][0])\n",
        "\n",
        "  keyword_sentence=' '.join(keyword)\n",
        "  return keyword_sentence"
      ],
      "metadata": {
        "id": "gr7w5FnzaRmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 도박사이트 분류 시작\n",
        "> 아래에 있는 url_list에 url을 넣어주세요\n",
        "\n",
        "> 이 셀 위에 있는 셀들은 첫 실행시에 한 번만 실행하면 됩니다"
      ],
      "metadata": {
        "id": "8TFa6xQNfg0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url_list=['https://www.snu.ac.kr/', 'https://newtoki.biz/manhwa', 'https://kkr-0708.com/?ref=1875']"
      ],
      "metadata": {
        "id": "3DQKYWzvk0PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_site_dict = {0: \"일반사이트\", 1: \"도박사이트\", 2: \"도박 제외 불법사이트\"}\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  text = []\n",
        "\n",
        "  for raw_url in url_list:\n",
        "    text.append(extraction(raw_url))\n",
        "\n",
        "\n",
        "  # 텍스트를 토큰화\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "  #입력에 대한 추론 (추론에서는 gradient 필요없음)\n",
        "  input_ids = inputs['input_ids']\n",
        "  attention_mask = inputs['attention_mask']\n",
        "\n",
        "  # 1단계 모델에 입력\n",
        "  output1 = model.distilbert1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "  logits1 = output1.logits\n",
        "\n",
        "  hidden1 = output1.hidden_states[-1]  # 마지막 레이어의 hidden state\n",
        "  output2 = model.distilbert2(inputs_embeds=hidden1, attention_mask=attention_mask)\n",
        "  logits2 = output2.logits\n",
        "  result = output2label(logits1, logits2, label_to_site_dict)\n",
        "\n",
        "\n",
        "\n",
        "  for keyword in text:\n",
        "    print(keyword)\n",
        "  print(f\"Result: {'/'.join(result)}\")"
      ],
      "metadata": {
        "id": "3-LIpDXTfZkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "faElQfkJD8v6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}