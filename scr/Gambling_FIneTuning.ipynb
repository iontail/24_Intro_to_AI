{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZId8y7N-aWe"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리"
      ],
      "metadata": {
        "id": "QVczo0w_1IhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QVNiaiNW15eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/MyDrive/2024_2학기 학교 데이터/Intro_to_AI_data\"\n",
        "\n",
        "raw_data = pd.read_csv('csrc_illegalSite_data_성균관대.csv')\n",
        "raw_data = raw_data[['keyword', 'label']]\n",
        "raw_data"
      ],
      "metadata": {
        "id": "Z26DA_kS1IU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data.isnull().sum()"
      ],
      "metadata": {
        "id": "dB_8IGf21INW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 레이블 개수 확인"
      ],
      "metadata": {
        "id": "E-Cw-9SylUPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_num = len(raw_data[\"label\"].unique())\n",
        "print(f'라벨 개수: {label_num}')\n",
        "print(f'데이터 라벨 종류: {raw_data[\"label\"].unique()}')\n",
        "\n",
        "print(f'각 라벨 개수: {raw_data[\"label\"].value_counts()}')"
      ],
      "metadata": {
        "id": "057DYg021H9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data['label'] = raw_data['label'] - 1\n",
        "\n",
        "label_to_site_dict = {0: \"일반사이트\", 1: \"도박사이트\", 2: \"도박 제외 불법사이트\"}\n",
        "\n",
        "print(f'각 라벨 개수: {raw_data[\"label\"].value_counts()}')"
      ],
      "metadata": {
        "id": "9zezK_7F1H5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "str_length = [len(untokenized_data) for untokenized_data in raw_data['keyword']]\n",
        "\n",
        "plt.hist(str_length, bins = 100)\n",
        "plt.title('Length of Untokenized Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DdC7z5CL6fpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(str_length)"
      ],
      "metadata": {
        "id": "SjIbNVqL1Hs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_data.shape)  # 전체 데이터의 크기 확인\n",
        "print(raw_data['keyword'].isnull().sum())  # text 열에 결측값이 있는지 확인\n",
        "print(raw_data['label'].isnull().sum())  # label 열에 결측값이 있는지 확인"
      ],
      "metadata": {
        "id": "HXm1xEzIQVvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_data['keyword'].shape)\n",
        "print(raw_data['label'].shape)"
      ],
      "metadata": {
        "id": "QifrfE2JUBuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터로더 정의"
      ],
      "metadata": {
        "id": "skLIizfPlaX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "\n",
        "# 데이터셋을 train/validation 세트로 나누기 (80%:20% 비율)\n",
        "\n",
        "# 허깅페이스 Dataset 포맷으로 변환\n",
        "dataset = Dataset.from_pandas(raw_data)\n",
        "\n",
        "\n",
        "# 전처리 함수 정의: Tokenization\n",
        "def preprocess_function(data):\n",
        "  return tokenizer(data['keyword'], truncation = True, padding = 'max_length', max_length = 512, return_tensors = 'pt')\n",
        "\n",
        "# one_hot encoding으로 변환\n",
        "def label2one_hot(data):\n",
        "  label = data['label']\n",
        "  one_hot = [0] * label_num\n",
        "  one_hot[label] = 1\n",
        "  data['label'] = one_hot\n",
        "  return data\n",
        "\n",
        "\n",
        "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "tokenized_data = dataset.map(preprocess_function, batched = True)\n",
        "#tokenized_data = tokenized_data.map(label2one_hot)\n",
        "\n",
        "# 'label' 컬럼을 'labels'로 변경 (허깅페이스)\n",
        "tokenized_data = tokenized_data.rename_column('label', \"labels\")\n",
        "tokenized_data = tokenized_data.rename_column('keyword', \"text\")\n",
        "\n",
        "# 데이터셋을 Trainer API로 사용하기 위해 필요한 포맷으로 설정\n",
        "tokenized_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "print(tokenized_data)"
      ],
      "metadata": {
        "id": "u2CS5o0d60U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_data = tokenized_data.train_test_split(test_size = 0.1)\n",
        "split_data"
      ],
      "metadata": {
        "id": "m7TyGoO3_SN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 확인\n",
        "sample = split_data['train'][0]['input_ids']\n",
        "sample_encoded = tokenizer.decode(sample, skip_special_tokens = True, clean_up_tokenization_spaces = True)\n",
        "\n",
        "print(tokenized_data, \"\\n\")\n",
        "print(sample)\n",
        "print(sample_encoded)"
      ],
      "metadata": {
        "id": "fYxe5NLA60aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer가 자동으로 맨 앞에 [CLS] 토큰 넣어줌\n",
        "\n",
        "tokenizer.cls_token_id"
      ],
      "metadata": {
        "id": "ROQ_A402Ho4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 아키텍쳐"
      ],
      "metadata": {
        "id": "WamuIjFf1JJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "i-kgXXv2-kaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "-1R3GWwO-keX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 허깅페이스 Token 설정"
      ],
      "metadata": {
        "id": "mTdOZ97VzoYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "from huggingface_hub import login, Repository\n",
        "\n",
        "# hugging-face 토큰\n",
        "# 절대 유출 금지\n",
        "access_token = ''\n",
        "login(access_token)\n",
        "\n",
        "# 허깅페이스 허브 연결\n",
        "#repo_name = 'iontail/TwoStageDistilBERT_Gambling'\n",
        "#repo = Repository(local_dir = 'TwoStageDistilBERT_Gambling', clone_from = repo_name)\n",
        "\n",
        "checkpoint_interval = 1 # 매 epoch마다 체크포인트 저장\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lzSVteve-khg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 버전 확인\n",
        "model_ver = 3\n",
        "\n",
        "save_model_path = \"/content/drive/MyDrive/2024_2학기 학교 데이터/Intro_to_AI_data/TwoStageDistilBERT_LoRA_ver\" + str(model_ver) + '.pt'\n",
        "\n",
        "save_hist_path = \"/content/drive/MyDrive/2024_2학기 학교 데이터/Intro_to_AI_data/hist_ver\" + str(model_ver) + '.pt'\n",
        "\n",
        "#체크포인트 저장 함수\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, loss, save_path):\n",
        "  checkpoint = {\n",
        "      \"model_state_dict\": model.state_dict(),\n",
        "      \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "      \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
        "      \"epoch\": epoch,\n",
        "      \"loss\": loss\n",
        "  }\n",
        "\n",
        "  torch.save(checkpoint, save_path)\n",
        "\n",
        "\n",
        "\n",
        "def save_hist(train_loss, train_acc, val_loss, val_acc, train_f1, val_f1, save_path):\n",
        "  hist_dict = {\n",
        "      \"train_loss_list\": train_loss,\n",
        "      \"train_acc_list\": train_acc,\n",
        "      \"val_loss_list\": val_loss,\n",
        "      \"val_acc_list\": val_acc,\n",
        "      \"train_f1_list\": train_f1,\n",
        "      \"val_f1_list\": val_f1\n",
        "  }\n",
        "\n",
        "  torch.save(hist_dict, save_path)\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, scheduler, file_path, hist_path):\n",
        "  checkpoint = torch.load(file_path)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  if scheduler:\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "\n",
        "  hist_checkpoint = torch.load(hist_path)\n",
        "  train_loss =  hist_checkpoint[\"train_loss_list\"]\n",
        "  train_acc = hist_checkpoint[\"train_acc_list\"]\n",
        "  val_loss = hist_checkpoint[\"val_loss_list\"]\n",
        "  val_acc = hist_checkpoint[\"val_acc_list\"]\n",
        "  train_f1 = hist_checkpoint(\"train_f1_list\")\n",
        "  val_f1 = hist_checkpoint(\"val_f1_list\")\n",
        "\n",
        "\n",
        "  print(f\"Checkpoint loaded from {file_path}, starting from epoch {epoch+1}\")\n",
        "  return model, optimizer, scheduler, epoch + 1, loss, train_loss, train_acc, val_loss, val_acc, train_f1, val_f1  # 불러올 때 다음 에포크부터 시작"
      ],
      "metadata": {
        "id": "611GUc_Lhv0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DistilBERT로 모델 불러오면 labels를 입력으로 받지 않기 때문에 DistilBertForSequenceClassification 사용\n",
        "\n",
        "from transformers import DistilBertForSequenceClassification, get_scheduler\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "\n",
        "class TwoStageDistilBERT_LoRA(nn.Module):\n",
        "  def __init__(self, distilbert_checkpoint, num_labels_1stage = 2, num_labels_2stage = 3):\n",
        "    super(TwoStageDistilBERT_LoRA, self).__init__()\n",
        "\n",
        "\n",
        "    # 첫 번째 stage\n",
        "    self.distilbert1 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_1stage, ignore_mismatched_sizes = True,\n",
        "                                                                           output_hidden_states=True)\n",
        "\n",
        "    lora_config1 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert1 = get_peft_model(self.distilbert1, lora_config1)\n",
        "\n",
        "    # 두 번째 stage\n",
        "    self.distilbert2 = DistilBertForSequenceClassification.from_pretrained(distilbert_checkpoint,\n",
        "                                                                           num_labels = num_labels_2stage, ignore_mismatched_sizes = True)\n",
        "\n",
        "    lora_config2 = LoraConfig(task_type = TaskType.SEQ_CLS, r = 8, lora_alpha = 32, target_modules = ['q_lin', 'v_lin'], lora_dropout = 0.1 )\n",
        "    self.distilbert2 = get_peft_model(self.distilbert2, lora_config2)\n",
        "\n",
        "\n",
        "  def forward(self, input_ids,  attention_mask, labels1 = None, labels2 = None):\n",
        "    output1 = self.distilbert1(input_ids = input_ids, attention_mask = attention_mask, labels = labels1)\n",
        "    hidden1 = output1.hidden_states[-1] # 마지막 레이어의 hidden state 가져오기\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    pred1 = torch.argmax(logits1, dim = 1)\n",
        "\n",
        "    output2 = self.distilbert2(inputs_embeds = hidden1, attention_mask = attention_mask, labels = labels2)\n",
        "    logits2 = output2.logits\n",
        "\n",
        "    total_loss = output1.loss + output2.loss\n",
        "\n",
        "\n",
        "    return total_loss, logits1, logits2\n",
        "\n",
        "  \"\"\"\n",
        "  #체크포인트 설정하여 저장 및 불러오기 위한 함수들 정의\n",
        "  def save_pretrained(self, save_directory):\n",
        "    os.makedirs(save_directory, exist_ok = True)\n",
        "\n",
        "    self.distilbert1.save_pretrained(os.path.join(save_directory, \"stage1\"))\n",
        "    torch.save(self.distilbert1.state_dict(), os.path.join(save_directory, \"stage1\", \"pytorch_model.bin\"))\n",
        "\n",
        "    self.distilbert2.save_pretrained(os.path.join(save_directory, \"stage2\"))\n",
        "    torch.save(self.distilbert2.state_dict(), os.path.join(save_directory, \"stage2\", \"pytorch_model.bin\"))\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, save_directory):\n",
        "      # 저장된 모델 불러오기\n",
        "      model = cls.__new__(cls)\n",
        "\n",
        "      model.distilbert1.load_state_dict(torch.load(os.path.join(save_directory, \"stage1\", \"pytorch_model.bin\")))\n",
        "      model.distilbert2.load_state_dict(torch.load(os.path.join(save_directory, \"stage2\", \"pytorch_model.bin\")))\n",
        "      return model\n",
        "\n",
        "\n",
        "  def get_hist(self):\n",
        "    pass\n",
        "  \"\"\"\n"
      ],
      "metadata": {
        "id": "4lkbXlbJ-kkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_DL = DataLoader(split_data['train'], shuffle = True, batch_size = batch_size)\n",
        "val_DL = DataLoader(split_data['test'], shuffle = True, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "viU6RDrC-kne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####################\n",
        "\"\"\"\n",
        "모델 중단되었으면 아래 변수를 'True'로 설정\n",
        "\"\"\"\n",
        "resume_training = False\n",
        "\n",
        "# warm-up step 확인\n",
        "# pre-trained 모델은 2%인데 너무 적어서 5%로 변경\n",
        "import math\n",
        "\n",
        "EPOCH = 20\n",
        "\n",
        "total_steps = EPOCH * len(train_DL)\n",
        "warm_up = int(total_steps * 0.05)\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "LAMBDA = 0.01\n",
        "\n",
        "model = TwoStageDistilBERT_LoRA(distilbert_checkpoint = checkpoint).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), weight_decay = LAMBDA, lr = learning_rate)\n",
        "scheduler = get_scheduler('linear', optimizer = optimizer, num_warmup_steps = warm_up, num_training_steps = total_steps)\n",
        "\n",
        "\n",
        "\n",
        "if resume_training:\n",
        "  model, optimizer, scheduler, start_epoch, last_loss, train_loss, train_acc, val_loss, val_acc, train_f1, val_f1 = load_checkpoint(model, optimizer, scheduler, save_model_path, save_hist_path)\n",
        "\n",
        "#model.to(DEVICE)\n",
        "model"
      ],
      "metadata": {
        "id": "Po4-neQU-kqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output2label(logits1, logits2, label_to_site_dict):\n",
        "  N = logits1.shape[0]\n",
        "  logits1 = np.array(logits1.cpu())\n",
        "  logits2 = np.array(logits2.cpu())\n",
        "\n",
        "\n",
        "\n",
        "  output1 = np.argmax(logits1, axis = 1)\n",
        "  #print(output1, \"\\n\",[label_to_site_dict[out] for out in output1])\n",
        "\n",
        "  mask = (output1 != 0)\n",
        "  output2 = np.argmax(logits2[mask], axis = 1)\n",
        "  #print(output2, \"\\n\", [label_to_site_dict[out] for out in output2])\n",
        "\n",
        "  result = []\n",
        "  result_label = []\n",
        "  idx = 0\n",
        "  for out in output1:\n",
        "    if out == 0:\n",
        "      result_label.append(out)\n",
        "      result.append(label_to_site_dict[out])\n",
        "    else:\n",
        "      result_label.append(output2[idx])\n",
        "      result.append(label_to_site_dict[output2[idx]])\n",
        "      idx += 1\n",
        "\n",
        "\n",
        "  print(result)\n",
        "  print(f\"Result: {result_label}\")\n",
        "\n",
        "def output2num(logits1, logits2):\n",
        "  N = logits1.shape[0]\n",
        "\n",
        "  logits1 = logits1.detach().cpu().numpy()\n",
        "  logits2 = logits2.detach().cpu().numpy()\n",
        "\n",
        "  output1 = np.argmax(logits1, axis = 1)\n",
        "  mask = (output1 != 0)\n",
        "  output2 = np.argmax(logits2[mask], axis = 1)\n",
        "\n",
        "  result = np.array([])\n",
        "  idx = 0\n",
        "  for out in output1:\n",
        "    if out == 0:\n",
        "      result = np.append(result, out)\n",
        "    else:\n",
        "      result = np.append(result, output2[idx])\n",
        "      idx += 1\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def get_metrics(logits1, logits2, labels2):\n",
        "\n",
        "  result = output2num(logits1, logits2)\n",
        "  labels2 = np.array(labels2.cpu())\n",
        "\n",
        "  correct = (result == labels2).sum().item()\n",
        "\n",
        "  TP_mask = (result == 1)\n",
        "  TP = (labels2[TP_mask] == 1).sum().item()\n",
        "\n",
        "  FP = (labels2[TP_mask] != 1).sum().item()\n",
        "\n",
        "  FN_mask = (result != 1)\n",
        "  FN = (labels2[FN_mask] == 1).sum().item()\n",
        "\n",
        "  TN = (labels2[FN_mask] != 1).sum().item()\n",
        "\n",
        "  ## correct ##\n",
        "\n",
        "  \"\"\"\n",
        "  # 예측값을 얻기 위해 logits에서 가장 높은 값을 선택\n",
        "  predictions1 = torch.argmax(logits1, dim = 1)\n",
        "  predictions2 = torch.argmax(logits2, dim = 1)\n",
        "\n",
        "  mask = (predictions1 == (labels1 == 0)) # 정상사이트는 정상사이트로 정상 분류했을 때 해당 자리는 1로 set\n",
        "  reverse_mask = (mask != 1) # 유해사이트로 분류시 해당 인덱스는 1로 set\n",
        "\n",
        "  correct1 = (mask == 1).sum().item()\n",
        "\n",
        "  # 첫 번째 Stage에서 유해사이트로 분류되었을 때,  2nd Stage에서의 분류 결과\n",
        "  correct2 = (predictions2[reverse_mask] == labels2[reverse_mask])\n",
        "  correct2 = correct2.sum().item()\n",
        "  acc = (correct1 + correct2)\n",
        "  \"\"\"\n",
        "\n",
        "  # 레이블이 3개여서 correct != TP + TN\n",
        "  return correct, TP, FP, FN, TN\n"
      ],
      "metadata": {
        "id": "2Idd3IGMZ3i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 테스트\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch in train_DL:\n",
        "    batch = {key: value.to(DEVICE) for key, value in batch.items()}\n",
        "\n",
        "    labels1 = batch['labels']  # 첫 번째 분류용 라벨\n",
        "    #labels1 = torch.where(torch.argmax(labels1, dim = 1) == 0, 0, 1) # Binary label 처리 (0이면 0, 나머지 1, 2이면 1)\n",
        "    labels1 = torch.where(labels1 ==0, 0, 1)\n",
        "    labels2 = batch['labels']\n",
        "\n",
        "\n",
        "    loss, logits1, logits2 = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'], labels1=labels1, labels2=labels2)\n",
        "    print(loss)\n",
        "    print(logits1)\n",
        "    print(labels2)\n",
        "\n",
        "    output2label(logits1, logits2, label_to_site_dict)\n",
        "    print(f\"Label: {labels2}\")\n",
        "    print(get_metrics(logits1, logits2, labels2))\n",
        "    print(\"-\"*60, \"\\n\")\n",
        "\n",
        "    #acc = get_acc(logits1, logits2, labels1, labels2)\n",
        "    #print(f\"Accuracy: {acc / batch_size}\")\n",
        "\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "id": "Au_MB4RAS8pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 추론 테스트"
      ],
      "metadata": {
        "id": "5IWU4gu-3Fho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# 입력 텍스트\n",
        "input_text = \"뉴스 국회 최근 합법화 국회의원 국민 헌법 법 제정\"\n",
        "# 텍스트를 토큰화\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# 입력에 대한 추론 (추론에서는 gradient 필요없음)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    input_ids = inputs['input_ids'].to(DEVICE)\n",
        "    attention_mask = inputs['attention_mask'].to(DEVICE)\n",
        "\n",
        "    # 1단계 모델에 입력\n",
        "    output1 = model.distilbert1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits1 = output1.logits\n",
        "\n",
        "    hidden1 = output1.hidden_states[-1]  # 마지막 레이어의 hidden state\n",
        "    output2 = model.distilbert2(inputs_embeds=hidden1, attention_mask=attention_mask)\n",
        "    logits2 = output2.logits\n",
        "    output2label(logits1, logits2, label_to_site_dict)\n"
      ],
      "metadata": {
        "id": "8mrcNwvKqtZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 파라미터 지정"
      ],
      "metadata": {
        "id": "LphIlFrERT39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파인튜닝 시작"
      ],
      "metadata": {
        "id": "mxtHKSMBdJaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early Stopping 커스텀 클래스 선언\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience = 3, min_delta = 0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        patience: 몇 번 연속으로 성능향상이 없을 때 종료할 것인지\n",
        "        min_delta: 성능 향상의 최소치\n",
        "    \"\"\"\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.counter = 0\n",
        "    self.best_loss = None\n",
        "    self.early_stop = False\n",
        "\n",
        "  def __call__(self, val_loss):\n",
        "    if self.best_loss is None:\n",
        "      self.best_loss = val_loss\n",
        "    elif val_loss < (self.best_loss - self.min_delta):\n",
        "      self.best_loss = val_loss\n",
        "      self.counter = 0\n",
        "    else:\n",
        "      self.counter +=1\n",
        "      if self.counter >= self.patience:\n",
        "        self.early_stop = True\n"
      ],
      "metadata": {
        "id": "jQ6-FFUCfFO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tuning(model, train_DL, val_DL, resume_training):\n",
        "\n",
        "  early_stopping = EarlyStopping(patience = 3)\n",
        "\n",
        "  if resume_training:\n",
        "    start = start_epoch\n",
        "    total_train_loss = train_loss\n",
        "    total_train_acc =  train_acc\n",
        "    total_val_loss = val_loss\n",
        "    total_val_acc = val_acc\n",
        "    total_train_f1 = train_f1\n",
        "    total_val_f1 = val_f1\n",
        "\n",
        "  else:\n",
        "    start = 0\n",
        "    total_train_loss = []\n",
        "    total_train_acc =  []\n",
        "    total_val_loss = []\n",
        "    total_val_acc = []\n",
        "    total_train_f1 = []\n",
        "    total_val_f1 = []\n",
        "\n",
        "  N = len(val_DL)\n",
        "  for epoch in range(start, EPOCH):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    val_loss_batch = 0\n",
        "    val_acc_batch = 0\n",
        "\n",
        "    TP_batch = 0\n",
        "    FP_batch = 0\n",
        "    FN_batch = 0\n",
        "\n",
        "    TP_batch_val = 0\n",
        "    FP_batch_val = 0\n",
        "    FN_batch_val = 0\n",
        "\n",
        "    tqdm_batch = tqdm(train_DL, desc = f\"Epoch {epoch + 1} / {EPOCH}\")\n",
        "    i = 1\n",
        "\n",
        "    for batch in tqdm_batch:\n",
        "      # acc 구하는데 사용 / interation\n",
        "\n",
        "\n",
        "      batch = {key: value.to(DEVICE) for key, value in batch.items()}\n",
        "      labels1 = batch['labels']  # 첫 번째 분류용 라벨\n",
        "      labels1 = torch.where(labels1 ==0, 0, 1)\n",
        "\n",
        "      labels2 = batch['labels']  # 두 번째 분류용 라벨\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss, logits1, logits2 = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'], labels1=labels1, labels2=labels2)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      total_loss += loss.item() * batch_size\n",
        "      tqdm_batch.set_postfix(loss = total_loss / i)\n",
        "      i += 1\n",
        "\n",
        "      batch_acc, TP, FP, FN, _ = get_metrics(logits1, logits2, labels2)\n",
        "      total_acc += batch_acc\n",
        "      TP_batch += TP\n",
        "      FP_batch += FP\n",
        "      FN_batch += FN\n",
        "\n",
        "\n",
        "    # 한 에폭마다 훈련 결과 저장\n",
        "    total_train_loss.append(total_loss/ len(train_DL))\n",
        "    total_train_acc.append(total_acc / (len(train_DL) * batch_size))\n",
        "    total_train_f1.append((2 * TP_batch) / (2 * TP_batch + FP_batch + FN_batch))\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    tqdm_batch_val = tqdm(val_DL, desc = f\"Validation\")\n",
        "    j = 1\n",
        "\n",
        "    for batch in tqdm_batch_val:\n",
        "\n",
        "\n",
        "      batch = {key: value.to(DEVICE) for key, value in batch.items()}\n",
        "      labels1 = batch['labels']  # 첫 번째 분류용 라벨\n",
        "      labels1 = torch.where(labels1 ==0, 0, 1)\n",
        "\n",
        "      labels2 = batch['labels']  # 두 번째 분류용 라벨\n",
        "\n",
        "      loss, logits1, logits2 = model(input_ids = batch['input_ids'], attention_mask = batch['attention_mask'], labels1=labels1, labels2=labels2)\n",
        "\n",
        "      val_loss_batch += loss.item() * batch_size\n",
        "      tqdm_batch_val.set_postfix(loss = val_loss_batch / j)\n",
        "      j +=1\n",
        "\n",
        "      batch_acc, TP, FP, FN, _ = get_metrics(logits1, logits2, labels2)\n",
        "      val_acc_batch += batch_acc\n",
        "      TP_batch_val += TP\n",
        "      FP_batch_val += FP\n",
        "      FN_batch_val += FN\n",
        "\n",
        "    total_val_loss.append(val_loss_batch / len(val_DL))\n",
        "    total_val_acc.append(val_acc_batch / (len(val_DL) * batch_size))\n",
        "    total_val_f1.append((2 * TP_batch_val) / (2 * TP_batch_val + FP_batch_val + FN_batch_val))\n",
        "\n",
        "    #조기종료\n",
        "    early_stopping(val_loss_batch / N)\n",
        "\n",
        "    if early_stopping.early_stop:\n",
        "      print(\"Early Stopping is Triggered!!!!\")\n",
        "      break\n",
        "    print(f'Accuracy: {val_acc_batch / (N * batch_size)}%')\n",
        "    save_checkpoint(model, optimizer, scheduler, epoch, loss, save_model_path)\n",
        "    save_hist(total_train_loss, total_train_acc, total_val_loss, total_val_acc, total_train_f1, total_val_f1, save_hist_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "YHatcrGxdJpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuning(model, train_DL, val_DL, resume_training)"
      ],
      "metadata": {
        "id": "imDaOo_ldJtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 훈련결과 시각화"
      ],
      "metadata": {
        "id": "h-TWVU5K7e7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist_data = torch.load(save_hist_path)\n",
        "\n",
        "train_loss =  hist_data[\"train_loss_list\"]\n",
        "train_acc = hist_data[\"train_acc_list\"]\n",
        "val_loss = hist_data[\"val_loss_list\"]\n",
        "val_acc = hist_data[\"val_acc_list\"]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# 첫 번째 그래프: Train Loss\n",
        "ax1.plot(range(1, len(train_loss) + 1), train_loss, label='Train', color='r')\n",
        "ax1.plot(range(1, len(val_loss) + 1), val_loss, label='Val', color='b')\n",
        "ax1.set_title('Loss')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "\n",
        "# 두 번째 그래프: Validation Loss\n",
        "ax2.plot(range(1, len(train_acc) + 1), train_acc, label='Train', color='r')\n",
        "ax2.plot(range(1, len(val_acc) + 1), val_acc, label='Val', color='b')\n",
        "ax2.set_title('Accuracy')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "\n",
        "# 레이아웃 자동 조정\n",
        "plt.tight_layout()\n",
        "\n",
        "# 그래프 출력\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qdZf-ZQAmBs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_f1 = hist_data[\"train_f1_list\"]\n",
        "val_f1 = hist_data[\"val_f1_list\"]\n",
        "\n",
        "plt.plot(range(1, len(train_f1) + 1), train_f1, label = 'Train', color = 'r')\n",
        "plt.plot(range(1, len(val_f1) + 1), val_f1, label = 'Val', color = 'b')\n",
        "plt.title(\"F1-Score\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N8nmZ5aLmBy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Validation Accuracy: {val_acc[-1]:.3%}\")\n",
        "print(f\"F1-Score: {val_f1[-1]:.3%}\")"
      ],
      "metadata": {
        "id": "RxyZVE2JMLt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Validation Accuracy: {max(val_acc):.3%}\")\n",
        "print(f\"F1-Score: {max(val_f1):.3%}\")"
      ],
      "metadata": {
        "id": "OcfvDJt1Nt1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_acc)"
      ],
      "metadata": {
        "id": "iGqIxdzWNYBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(int(33484/(209 + 555 + 485)))"
      ],
      "metadata": {
        "id": "EbCpKvEAOJZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(100 * 9546 / 9786, \"%\")"
      ],
      "metadata": {
        "id": "MfBegUbCVtLS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}